{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/1400/1*bSLNlG7crv-p-m4LVYYk3Q.png\" width=\"400px\" height=\"100px\">","metadata":{}},{"cell_type":"markdown","source":"## 1. What is YOLO?<br>\n\n Yolo is a state-of-the-art, object detection algorithm. It was developed by Joseph Redmon. The biggest advantage over other architectures is that speed without much compramising in accuracy. The Yolo model family models are really fast, much faster than R-CNN and others.In this article we will go through the third version of yolo yolov3 its architecture, how it works and its implementation in pytorch.","metadata":{}},{"cell_type":"markdown","source":"## 2. How Yolo Works? <br>\n\nAs yolo works with one look at an image,sliding window is not a good approach for it.Instead the entire image can be splitted in to grids of size SxS cells.Now each cell will be responsible for predicting different things.\n\nTypically, as is the case for all object detectors the features learned by the convolutional layers are passed onto a classifier/regressor which makes the detection prediction.ie,coordinates of the bounding boxes, the class label.. etc but in YOLO, the prediction is done by using a convolutional layer which uses 1 x 1 convolutions.\n\nThe important thing to note is that the output is a feature map.Each cell of these feature map predicts different things.Let me explain it in detail.Each cell will predict 4 coordinates for bounding box,probability of bounding box and class probabilities for each of the class.Here probability of bounding box means probability that a box contains an object. In case that there is no object in some grid cell, it is important that confidence value is very low for that cell.For COCO dataset we have 80 classes.So in total each cell will predict 85 values.\n\nIn Yolo, anchor boxes are used to predict bounding boxes. The main idea of anchor boxes is to predefine two different shapes. They are called anchor boxes or anchor box shapes. In this way, we will be able to associate two predictions with the two anchor boxes. In general, we might use even more anchor boxes (five or even more). Anchors were calculated on the COCO dataset using k-means clustering. \n\nSo, we have (B x (5 + C)) entries in the feature map. B represents the number of bounding boxes each cell can predict. Each of the bounding boxes have 5 + C attributes, which describe the center coordinates, the dimensions, the bounding box probability score and C class probability for each bounding box. YOLO v3 predicts 3 bounding boxes for every cell. ie, technically each cell have 3 anchors.therefore 3 bounding boxes.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://blog.paperspace.com/content/images/2018/04/yolo-5.png\" width=\"400px\" height=\"100px\">","metadata":{}},{"cell_type":"markdown","source":"\nFor each bounding box, We have<br>\n* 4 cordinates (tx,ty,tw,th)<br>\n* Probability of bounding box. ie,probability that an object is present inside the bounding box<br>\n* Class probabilities for each class (Here, 80 classes)<br>\n\nIf there is some offset from the top left corner by cx,cy, then the predictions correspond to:\n\n## **bx = σ(tx) + cx**<br>\n\n## **by = σ(ty) + cy**<br>\n\n## **bw = pw x e^tw**<br>\n\n## **bh = ph x e^th**<br>\n\nHere bx, by, bw, bh are the x,y center co-ordinates, width and height of our prediction. tx, ty, tw, th are the network outputs. cx and cy are the top-left co-ordinates of the grid. pw and ph are anchors dimensions for the box.Before going further we will explain about the centre cordinates.\n\n\n### Centre coordinates and boundingbox dimensions\nActually we are running centre coordinate prediction through a sigmoid function. so the value of the output to be between 0 and 1. Why should this be the case? Did you ever thought about that.\n\nNormally yolo doesn't directly predict the bounding boxes.It predicts the offsets relative to the top left corner of the grid cell which is predicting the object. Also it is normalised by the dimensions of the cell from the feature map, which is, 1.\n\nFor example, consider the case of our dog image. If the prediction for center is (0.5, 0.3).Then the  actual centre cordinate lies at (6.5,6.3) in 13x13 feature map.similarly what if the prediction for center is (1.4,0.9) then the actual centre cordinate lies at (7.4,6.9).This is opposite to concept of YOLO. That is why the output is passed through a sigmoid function so it becomes in range 0 to 1.\n\nThe dimensions of the bounding box are predicted by applying a log space transform to output and then multiplying with an anchor.The resultant predictions, bw and bh, are normalised by the height and width of the anchors.So, if the predictions bx and by for the box containing the dog are (0.7, 0.8), then the actual width and height on if we have anchor box of height 10 and width 8 is is (10 x 0.7, 8 x 0.8).\n\n## Predictions\n\nIn one pass we can go from an input image to the output tensor which corresponds to the detections for the image. Also, it is worth mentioning that YOLOv3 predicts boxes at 3 different scales.In the above figure it is represented by y1,y2 and y3.\n\nUsing the COCO dataset, YOLOv3 predicts 80 different classes. YOLO outputs bounding boxes and class prediction as well. If we split an image into a 13 x 13 grid of cells and use 3 anchors box, the total output prediction is 13 x 13 x 3 or 169 x 3. However, YOLOv3 uses 3 different prediction scales which splits an image into (13 x 13), (26 x 26) and (52 x 52) grid of cells and with 3 anchors for each scale. So, the total output prediction will be((52 x 52) + (26 x 26) + 13 x 13)) x 3 = 10647 bounding boxes.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://s3-eu-west-1.amazonaws.com/ppreviews-plos-725668748/15596051/preview.jpg\" width=\"1000px\" height=\"500px\">","metadata":{}},{"cell_type":"markdown","source":"## Non-Max suppression\n\nSuppose N is the threshold and we have two lists A and B.Then for each class detected in image, \n\n* Select the proposal with highest confidence score, remove it from A and add it to the final proposal list B. (Initially B is empty).\n* Now compare this proposal with all the proposals by calculating the IOU (Intersection over Union) of this proposal with every other proposal. If the IOU is greater than the threshold N, remove that proposal from A.\n* Again take the proposal with the highest confidence from the remaining proposals in A and remove it from A and add it to B.\n* Once again calculate the IOU of this proposal with all the proposals in A and eliminate the boxes which have high IOU than threshold.\n* This process is repeated until there are no more proposals left in A.\n\n\n\n<img src=\"https://miro.medium.com/max/1400/1*6d_D0ySg-kOvfrzIRwHIiA.png\" width=\"800px\" height=\"300px\">\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Before getting to implimentation part if you have any difficulties in understanding concepts Andrew ng's [tutorial](http://https://www.youtube.com/watch?v=9s_FpMpdYW8) on youtube will be helpful to you.\n\nNow we will get to the implementation part using pytorch.First we will import necssary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport cv2\nimport os\nimport time\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n# import wget","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-14T08:25:27.475987Z","iopub.execute_input":"2022-08-14T08:25:27.476287Z","iopub.status.idle":"2022-08-14T08:25:29.281900Z","shell.execute_reply.started":"2022-08-14T08:25:27.476258Z","shell.execute_reply":"2022-08-14T08:25:29.281113Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"weightsfile = '/kaggle/input/data-for-yolo-v3-kernel/yolov3.weights'\nclassfile = '/kaggle/input/data-for-yolo-v3-kernel/coco.names'\ncfgfile = '/kaggle/working/yolov3.cfg'\nsample_img1 = '/kaggle/working/input/dog-cycle-car.png'\ninput_dir = '/kaggle/working/input'\noutput_dir = '/kaggle/working/output'\nnms_thesh = 0.5","metadata":{"execution":{"iopub.status.busy":"2022-08-14T08:25:51.847684Z","iopub.execute_input":"2022-08-14T08:25:51.848057Z","iopub.status.idle":"2022-08-14T08:25:51.852615Z","shell.execute_reply.started":"2022-08-14T08:25:51.848020Z","shell.execute_reply":"2022-08-14T08:25:51.851955Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(input_dir):\n    os.mkdir(input_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n! wget wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n! mv dog-cycle-car.png /kaggle/working/input\n! cp /kaggle/input/data-for-yolo-v3-kernel/office.jpg /kaggle/working/input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create the model using the configuration file.First we will parse it and then feed to our model","metadata":{}},{"cell_type":"code","source":"def parse_cfg(config_file):\n    file = open(config_file,'r')\n    file = file.read().split('\\n')\n    file =  [line for line in file if len(line)>0 and line[0] != '#']\n    file = [line.lstrip().rstrip() for line in file]\n\n    final_list = []\n    element_dict = {}\n    for line in file:\n\n        if line[0] == '[':\n            if len(element_dict) != 0:     # appending the dict stored on previous iteration\n                    final_list.append(element_dict)\n                    element_dict = {} # again emtying dict\n            element_dict['type'] = ''.join([i for i in line if i != '[' and i != ']'])\n            \n        else:\n            val = line.split('=')\n            element_dict[val[0].rstrip()] = val[1].lstrip()  #removing spaces on left and right side\n        \n    final_list.append(element_dict) # appending the values stored for last set\n    return final_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DummyLayer(nn.Module):\n    def __init__(self):\n        super(DummyLayer, self).__init__()\n        \n\n        \nclass DetectionLayer(nn.Module):\n    def __init__(self, anchors):\n        super(DetectionLayer, self).__init__()\n        self.anchors = anchors\n        \n        \n\ndef create_model(blocks):\n#     blocks = parse_cfg(cfgfile)\n    darknet_details = blocks[0]\n    channels = 3 \n    output_filters = []\n    modulelist = nn.ModuleList()\n    \n    for i,block in enumerate(blocks[1:]):\n        seq = nn.Sequential()\n        if (block[\"type\"] == \"convolutional\"):\n            activation = block[\"activation\"]\n            filters = int(block[\"filters\"])\n            kernel_size = int(block[\"size\"])\n            strides = int(block[\"stride\"])\n            use_bias= False if (\"batch_normalize\" in block) else True\n            pad = (kernel_size - 1) // 2\n            \n            conv = nn.Conv2d(in_channels=channels, out_channels=filters, kernel_size=kernel_size, \n                             stride=strides, padding=pad, bias = use_bias)\n            seq.add_module(\"conv_{0}\".format(i), conv)\n            \n            if \"batch_normalize\" in block:\n                bn = nn.BatchNorm2d(filters)\n                seq.add_module(\"batch_norm_{0}\".format(i), bn)\n\n            if activation == \"leaky\":\n                activn = nn.LeakyReLU(0.1, inplace = True)\n                seq.add_module(\"leaky_{0}\".format(i), activn)\n            \n        elif (block[\"type\"] == \"upsample\"):\n            upsample = nn.Upsample(scale_factor = 2, mode = \"bilinear\")\n            seq.add_module(\"upsample_{}\".format(i), upsample)\n        \n        elif (block[\"type\"] == 'route'):\n            # start and end is given in format (eg:-1 36 so we will find layer number from it.\n            # we will find layer number in negative format\n            # so that we can get the number of filters in that layer\n            block['layers'] = block['layers'].split(',')\n            block['layers'][0] = int(block['layers'][0])\n            start = block['layers'][0]\n            if len(block['layers']) == 1:               \n                filters = output_filters[i + start]\n                       \n            \n            elif len(block['layers']) > 1:\n                block['layers'][1] = int(block['layers'][1]) - i \n                end = block['layers'][1]\n                filters = output_filters[i + start] + output_filters[i + end]\n                  \n            \n            route = DummyLayer()\n            seq.add_module(\"route_{0}\".format(i),route)\n                \n      \n        elif block[\"type\"] == \"shortcut\":\n            from_ = int(block[\"from\"])\n            shortcut = DummyLayer()\n            seq.add_module(\"shortcut_{0}\".format(i),shortcut)\n            \n            \n        elif block[\"type\"] == \"yolo\":\n            mask = block[\"mask\"].split(\",\")\n            mask = [int(m) for m in mask]\n            anchors = block[\"anchors\"].split(\",\")\n            anchors = [(int(anchors[i]), int(anchors[i + 1])) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in mask]\n            block[\"anchors\"] = anchors\n            \n            detectorLayer = DetectionLayer(anchors)\n            seq.add_module(\"Detection_{0}\".format(i),detectorLayer)\n                \n        modulelist.append(seq)\n        output_filters.append(filters)  \n        channels = filters\n    \n    return darknet_details, modulelist\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def prediction(x,inp_dim,anchors,num_classes,CUDA=False):\n    # x --> 4D feature map\n    batch_size = x.size(0)\n    grid_size = x.size(2)\n    stride =  inp_dim // x.size(2)   # factor by which current feature map reduced from input\n#     grid_size = inp_dim // stride\n    \n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n# \n    prediction = x.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n    \n    # the dimension of anchors is wrt original image.We will make it corresponding to feature map\n    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n    #Add the center offsets\n    grid = np.arange(grid_size)\n    a,b = np.meshgrid(grid, grid)\n\n    x_offset = torch.FloatTensor(a).view(-1,1) #(1,gridsize*gridsize,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n\n    if CUDA:\n        x_offset = x_offset.cuda()\n        y_offset = y_offset.cuda()\n\n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n\n    prediction[:,:,:2] += x_y_offset\n\n    #log space transform height and the width\n    anchors = torch.FloatTensor(anchors)\n\n    if CUDA:\n        anchors = anchors.cuda()\n\n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors #width and height\n    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))    \n    prediction[:,:,:4] *= stride    \n    return prediction\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n        self.blocks = parse_cfg(cfgfile)\n        self.net_info, self.module_list = create_model(self.blocks)\n        \n    def forward(self, x, CUDA=False):\n        modules = self.blocks[1:]\n        outputs = {}   #We cache the outputs for the route layer\n        write = 0     #This is explained a bit later\n        for i, module in enumerate(modules):        \n            module_type = (module[\"type\"])\n            if module_type == \"convolutional\" or module_type == \"upsample\":\n                x = self.module_list[i](x)\n                outputs[i] = x\n                \n            elif module_type == \"route\":\n                layers = module[\"layers\"]\n                layers = [int(a) for a in layers]\n                if len(layers) == 1:\n                    x = outputs[i + layers[0]]\n                if len(layers) > 1:\n                    map1 = outputs[i + layers[0]]\n                    map2 = outputs[i + layers[1]]\n                    x = torch.cat((map1,map2),1)\n       \n                outputs[i] = x\n                \n            elif  module_type == \"shortcut\":\n                from_ = int(module[\"from\"])\n\n                # just adding outputs for residual network\n                x = outputs[i-1] + outputs[i+from_]  \n                outputs[i] = x\n                \n            elif module_type == 'yolo':\n                anchors = self.module_list[i][0].anchors\n                \n                #Get the input dimensions\n                inp_dim = int(self.net_info[\"height\"])\n                #Get the number of classes\n                num_classes = int(module[\"classes\"])\n            \n                #Transform \n                x = x.data   # get the data at that point\n                x = prediction(x,inp_dim,anchors,num_classes)\n                \n                if not write:              #if no collector has been intialised. \n                    detections = x\n                    write = 1\n                else:       \n                    detections = torch.cat((detections, x), 1)\n\n                outputs[i] = outputs[i-1]\n                \n        try:\n            return detections   #return detections if present\n        except:\n            return 0\n     \n    def load_weights(self, weightfile):\n        \n        #Open the weights file\n        fp = open(weightfile, \"rb\")\n\n        #The first 4 values are header information \n        # 1. Major version number\n        # 2. Minor Version Number\n        # 3. Subversion number \n        # 4. IMages seen \n        header = np.fromfile(fp, dtype = np.int32, count = 5)\n        self.header = torch.from_numpy(header)\n        self.seen = self.header[3]\n        \n        #The rest of the values are the weights\n        # Let's load them up\n        weights = np.fromfile(fp, dtype = np.float32)\n        \n        ptr = 0\n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i + 1][\"type\"]\n            \n            if module_type == \"convolutional\":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n                except:\n                    batch_normalize = 0\n                \n                conv = model[0]\n                \n                if (batch_normalize):\n                    bn = model[1]\n                    \n                    #Get the number of weights of Batch Norm Layer\n                    num_bn_biases = bn.bias.numel()\n                    \n                    #Load the weights\n                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    \n                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    #Cast the loaded weights into dims of model weights. \n                    bn_biases = bn_biases.view_as(bn.bias.data)\n                    bn_weights = bn_weights.view_as(bn.weight.data)\n                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n                    bn_running_var = bn_running_var.view_as(bn.running_var)\n\n                    #Copy the data to model\n                    bn.bias.data.copy_(bn_biases)\n                    bn.weight.data.copy_(bn_weights)\n                    bn.running_mean.copy_(bn_running_mean)\n                    bn.running_var.copy_(bn_running_var)\n                \n                else:\n                    #Number of biases\n                    num_biases = conv.bias.numel()\n                \n                    #Load the weights\n                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n                    ptr = ptr + num_biases\n                    \n                    #reshape the loaded weights according to the dims of the model weights\n                    conv_biases = conv_biases.view_as(conv.bias.data)\n                    \n                    #Finally copy the data\n                    conv.bias.data.copy_(conv_biases)\n                    \n                    \n                #Let us load the weights for the Convolutional layers\n                num_weights = conv.weight.numel()\n                \n                #Do the same as above for weights\n                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n                ptr = ptr + num_weights\n\n                conv_weights = conv_weights.view_as(conv.weight.data)\n                conv.weight.data.copy_(conv_weights)\n                # Note: we dont have bias for conv when batch normalization is there","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"Now we will write some utility functions regarding iou calculation,nan max supression etc","metadata":{}},{"cell_type":"code","source":"def bbox_iou(box1, box2):\n    \"\"\"\n    Returns the IoU of two bounding boxes \n    \n    \"\"\"\n    #Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n    \n    #get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n    \n    #Intersection area\n    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n \n    #Union Area\n    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n    \n    iou = inter_area / (b1_area + b2_area - inter_area)\n    \n    return iou\n\n\ndef unique(tensor):\n    tensor_np = tensor.cpu().numpy()\n    unique_np = np.unique(tensor_np)\n    unique_tensor = torch.from_numpy(unique_np)\n    \n    tensor_res = tensor.new(unique_tensor.shape)\n    tensor_res.copy_(unique_tensor)\n    return tensor_res\n\ndef write_results(prediction, confidence, num_classes, nms_conf = 0.4):\n    # taking only values above a particular threshold and set rest everything to zero\n    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n    prediction = prediction*conf_mask\n    \n    \n    #(center x, center y, height, width) attributes of our boxes, \n    #to (top-left corner x, top-left corner y, right-bottom corner x, right-bottom corner y)\n    box_corner = prediction.new(prediction.shape)\n    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_corner[:,:,:4]\n    \n    batch_size = prediction.size(0)\n    write = False\n    \n    # we can do non max suppression only on individual images so we will loop through images\n    for ind in range(batch_size):  \n        image_pred = prediction[ind] \n        # we will take only those rows with maximm class probability\n        # and corresponding index\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.float().unsqueeze(1)\n        max_conf_score = max_conf_score.float().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        # concatinating index values and max probability with box cordinates as columns\n        image_pred = torch.cat(seq, 1) \n        #Remember we had set the bounding box rows having a object confidence\n        # less than the threshold to zero? Let's get rid of them.\n        non_zero_ind =  (torch.nonzero(image_pred[:,4])) # non_zero_ind will give the indexes \n        image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n        try:\n            #Get the various classes detected in the image\n            img_classes = unique(image_pred_[:,-1]) # -1 index holds the class index\n        except:\n             continue\n       \n        for cls in img_classes:\n            #perform NMS\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n            \n            # sort them based on probability\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]#getting index\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n            \n            for i in range(idx):\n                #Get the IOUs of all boxes that come after the one we are looking at \n                 #in the loop\n                try:\n                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n                except ValueError:\n                    break\n                except IndexError:\n                    break\n                \n                #Zero out all the detections that have IoU > treshhold\n                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n                image_pred_class[i+1:] *= iou_mask\n                \n                #Remove the non-zero entries\n                non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n          \n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n            \n            #creating a row with index of images\n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n    \n    return output\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detection on unseen images","metadata":{}},{"cell_type":"code","source":"# Utility functions\n\n# function to load the classes\ndef load_classes(namesfile):\n    fp = open(namesfile, \"r\")\n    names = fp.read().split(\"\\n\")[:-1]\n    return names\n\n# function converting images from opencv format to torch format\ndef prep_image(img, inp_dim):\n    \"\"\"\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    \"\"\"\n\n    orig_im = cv2.imread(img)\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\n#function letterbox_image that resizes our image, keeping the \n# aspect ratio consistent, and padding the left out areas with the color (128,128,128)\ndef letterbox_image(img, inp_dim):\n    '''resize image with unchanged aspect ratio using padding'''\n    img_w, img_h = img.shape[1], img.shape[0]\n    w, h = inp_dim\n    new_w = int(img_w * min(w/img_w, h/img_h))\n    new_h = int(img_h * min(w/img_w, h/img_h))\n    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n    \n    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n\n    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,:] = resized_image\n    \n    return canvas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CUDA = False\nbatch_size = 2\n#Set up the neural network\nprint(\"Loading network.....\")\nmodel = Darknet(cfgfile)\nmodel.load_weights(weightsfile)\nprint(\"Network successfully loaded\")\nclasses = load_classes(classfile)\nprint('Classes loaded')\ninp_dim = int(model.net_info[\"height\"])\nassert inp_dim % 32 == 0 \nassert inp_dim > 32\n\n#If there's a GPU availible, put the model on GPU\nif CUDA:\n    model.cuda()\n\n#Set the model in evaluation mode\nmodel.eval()\n\n\n# read images from folder 'images' or direcly  image\nread_dir = time.time()\n#Detection phase\ntry:\n    imlist = [os.path.join(os.path.realpath('.'), input_dir, img) for img in os.listdir(input_dir)]\nexcept NotADirectoryError:\n    imlist = []\n    imlist.append(os.path.join(os.path.realpath('.'), input_dir))\nexcept FileNotFoundError:\n    print (\"No file or directory with the name {}\".format(input_dir))\n    exit()\n    \nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    \nload_batch = time.time()\n\n# preparing list of loaded images\n# [[image,original_image,dim[0],dim[1]]]\nbatches = list(map(prep_image, imlist, [inp_dim for x in range(len(imlist))]))\nim_batches = [x[0] for x in batches] # list of resized images\norig_ims = [x[1] for x in batches] # list of original images\nim_dim_list = [x[2] for x in batches] # dimension list\nim_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2) #repeating twice\n    \n    \nif CUDA:\n    im_dim_list = im_dim_list.cuda()\n\n    \n# converting image to batches    \nreminder = 0\nif (len(im_dim_list) % batch_size): #if reminder is there, reminder = 1\n    reminder = 1\n\nif batch_size != 1:\n    num_batches = len(imlist) // batch_size + reminder            \n    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,len(im_batches))])) \n                 for i in range(num_batches)] \n    \n    \ni = 0\nwrite = False\n    \nobjs = {}    \nfor batch in im_batches:\n        #load the image \n        start = time.time()\n        if CUDA:\n            batch = batch.cuda()       \n        #Apply offsets to the result predictions\n        #Tranform the predictions as described in the YOLO paper\n        #flatten the prediction vector \n        # B x (bbox cord x no. of anchors) x grid_w x grid_h --> B x bbox x (all the boxes) \n        # Put every proposed box as a row.\n        with torch.no_grad():\n            prediction = model(Variable(batch), CUDA)\n        \n        prediction = write_results(prediction, confidence=0.5, num_classes=80, nms_conf = nms_thesh)\n        \n        if type(prediction) == int:\n            i += 1\n            continue\n\n\n        prediction[:,0] += i*batch_size\n                  \n        if not write:\n            output = prediction\n            write = 1\n        else:\n            output = torch.cat((output,prediction))  # concating predictions from each batch\n        i += 1\n        \n        if CUDA:\n            torch.cuda.synchronize()\n    \ntry:\n    output\nexcept NameError:\n    print(\"No detections were made\")\n    exit()\n\n    \n#Before we draw the bounding boxes, the predictions contained in our output tensor \n#are predictions on the padded image, and not the original image. Merely, re-scaling them \n#to the dimensions of the input image won't work here. We first need to transform the\n#co-ordinates of the boxes to be measured with respect to boundaries of the area on the\n#padded image that contains the original image\n\n\nim_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\nscaling_factor = torch.min(inp_dim/im_dim_list,1)[0].view(-1,1)\noutput[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\noutput[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\noutput[:,1:5] /= scaling_factor\n    \nfor i in range(output.shape[0]):\n    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n        \n            \n\ndef write(x, batches, results):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    img = results[int(x[0])]\n    cls = int(x[-1])\n    label = \"{0}\".format(classes[cls])\n    color = (0,0,255)\n    cv2.rectangle(img, c1, c2,color, 2)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1)\n    return img\n    \n            \nlist(map(lambda x: write(x, im_batches, orig_ims), output))\n      \ndet_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(output_dir,x.split(\"/\")[-1]))\n    \nlist(map(cv2.imwrite, det_names, orig_ims))\n    \n\ntorch.cuda.empty_cache()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('/kaggle/working/output/det_dog-cycle-car.png') \nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,10))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T08:26:54.335935Z","iopub.execute_input":"2022-08-14T08:26:54.336235Z","iopub.status.idle":"2022-08-14T08:26:54.471549Z","shell.execute_reply.started":"2022-08-14T08:26:54.336206Z","shell.execute_reply":"2022-08-14T08:26:54.470232Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('/kaggle/working/output/det_office.jpg') \nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,10))\nplt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}